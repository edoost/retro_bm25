{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retro[on] - Retro[off] loss diff analysis\n",
    "\n",
    "In this analysis we will investigate whether the difference in loss (i.e., with vs. without retrieval) correlates with the number of tokens that overlap (between retrieved and to-be-predicted tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('retro/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from train_retro import get_retro_dataset_from_spec, retro_collate_fn, RetroModelLMHeadLightning\n",
    "from modeling_retro import RetroConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retro_on_config = RetroConfig(**json.load(Path('retro/data/model/retro.json').open()))\n",
    "\n",
    "val_ds = get_retro_dataset_from_spec(\n",
    "    spec_file=Path('retro/data/datasets/MassiveOpenText/val_sentence_transformer_neighbours.spec.json'),\n",
    "    num_neighbours=2,\n",
    "    continuation_chunks=1,\n",
    "    pad_token_idx=retro_on_config.pad_token_idx,\n",
    "    max_len=1024\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, num_workers=4)\n",
    "print(f\"Total size of validation set: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram token overlap\n",
    "The unigram overlap between the input and its neighbors (and contunation chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_retro import RetroTrainingExample\n",
    "\n",
    "overlap_ds = get_retro_dataset_from_spec(\n",
    "    spec_file=Path('retro/data/datasets/MassiveOpenText/val_sentence_transformer_neighbours.spec.json'),\n",
    "    num_neighbours=2,\n",
    "    continuation_chunks=1,\n",
    "    pad_token_idx=retro_on_config.pad_token_idx,\n",
    "    max_len=1024\n",
    ")\n",
    "\n",
    "\n",
    "def get_attending_chunk_neighbour_unigram_overlap_for_sequence(ex: RetroTrainingExample, pad_token_idx=0):\n",
    "    num_chunks = ex.neighbour_ids.shape[0]\n",
    "    num_neighbours = ex.neighbour_ids.shape[1]\n",
    "    neighbour_size = ex.neighbour_ids.shape[2]\n",
    "    input_ids = ex.input_ids.numpy().reshape(num_chunks, -1)\n",
    "    chunk_size = input_ids.shape[1]\n",
    "    neighbour_ids = ex.neighbour_ids.numpy()\n",
    "    \n",
    "    if num_chunks == 1:\n",
    "        return []\n",
    "\n",
    "    overlaps = np.zeros(num_chunks - 1)\n",
    "\n",
    "    for i in range(1, num_chunks):\n",
    "        first_pad_index_of_chunk = np.nonzero(input_ids[i, :] == pad_token_idx)[0]\n",
    "        chunk_len = first_pad_index_of_chunk[0] if first_pad_index_of_chunk.size > 0 else chunk_size\n",
    "        overlaps[i-1] = np.in1d(input_ids[i, :chunk_len], neighbour_ids[i-1]).sum()\n",
    "    return overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "u_overlaps = []\n",
    "for i, ex in tqdm(enumerate(overlap_ds), total=len(overlap_ds)):\n",
    "    u_overlaps.extend(get_attending_chunk_neighbour_unigram_overlap_for_sequence(ex))  # Average over input chunks [num neighbours, chunk size]\n",
    "\n",
    "print(len(u_overlaps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "The cosine similarity between the input and its neighbors (and contunation chunks) in terms of sentence transformers representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_retro import ChunkedSequenceDataset, RetroTrainingExample, ShardedChunkedSequenceDataset, \\\n",
    "    ChunkNeighbourDataset, ShardedChunkNeighbourDataset\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.spatial.distance import cosine\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ChunkedSequenceDatasetWithIDGetter(ChunkedSequenceDataset):\n",
    "    def get_chunk_ids(self, chunk_index, shard_range_start, include_continuation_chunks=0):\n",
    "        start_idx = chunk_index\n",
    "        end_idx = chunk_index + 1\n",
    "        while end_idx - start_idx - 1 < include_continuation_chunks and \\\n",
    "            end_idx < len(self.chunk2seq) and \\\n",
    "            self.chunk2seq[start_idx] == self.chunk2seq[end_idx]:\n",
    "            end_idx += 1\n",
    "        return slice(start_idx+shard_range_start, end_idx+shard_range_start)\n",
    "        \n",
    "        \n",
    "class ShardedChunkedSequenceDatasetWithIDGetter(ShardedChunkedSequenceDataset):  \n",
    "    def get_chunk_ids(self, chunk_index, include_continuation_chunks: int=0):\n",
    "        for shard_range, shard in zip(self.shard_chunk_ranges, self.shards):\n",
    "            if int(chunk_index) in shard_range:\n",
    "                local_chunk_index = chunk_index - shard_range.start\n",
    "                return shard.get_chunk_ids(local_chunk_index, shard_range.start, include_continuation_chunks)\n",
    "        raise IndexError(f\"Chunk with index {chunk_index} not found in index\")        \n",
    "\n",
    "        \n",
    "class ChunkNeighbourDatasetWithIDGetter(ChunkNeighbourDataset):\n",
    "    def get_neighbours_ids(self, chunk_index: int, num_neighbours: int=None, continuation_chunks: int=1):\n",
    "        return [self.retrieval_dataset.get_chunk_ids(neighbour_chunk_idx, continuation_chunks) \\\n",
    "                if neighbour_chunk_idx != -1 else None \\\n",
    "                for neighbour_chunk_idx in self.neighbours[chunk_index][:num_neighbours]]\n",
    "\n",
    "    \n",
    "class ShardedChunkNeighbourDatasetWithIDGetter(ShardedChunkNeighbourDataset):\n",
    "    def get_neighbours_ids(self, chunk_index: int, num_neighbours: int=None, continuation_chunks: int=1):\n",
    "        for shard_range, shard in zip(self.shard_ranges, self.shards):\n",
    "            if int(chunk_index) in shard_range:\n",
    "                local_index = chunk_index - shard_range.start\n",
    "                return shard.get_neighbours_ids(local_index, num_neighbours, continuation_chunks)\n",
    "        raise IndexError(f\"Neighbours for index {chunk_index} not found\")    \n",
    "    \n",
    "                \n",
    "class RetroEmbeddingDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dataset: ShardedChunkedSequenceDataset, \n",
    "        neighbour_dataset: ShardedChunkNeighbourDatasetWithIDGetter, \n",
    "        val_embeddings,\n",
    "        ret_embeddings,\n",
    "        num_neighbours=None, \n",
    "        continuation_chunks=1, \n",
    "        pad_token_idx=0,\n",
    "        max_len=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dataset = input_dataset\n",
    "        self.neighbour_dataset = neighbour_dataset\n",
    "        self.num_neighbours = num_neighbours\n",
    "        self.continuation_chunks = continuation_chunks\n",
    "        self.neighbour_size = neighbour_dataset.chunk_size * (1 + continuation_chunks)\n",
    "        self.pad_token_idx = pad_token_idx\n",
    "        self.max_num_chunks = max_len // input_dataset.chunk_size if max_len is not None else None\n",
    "        self.val_embeddings = val_embeddings\n",
    "        self.ret_embeddings = ret_embeddings\n",
    "\n",
    "        if max_len is not None:\n",
    "            assert max_len % input_dataset.chunk_size == 0, \\\n",
    "                \"max_len must be a multiple of chunk_size\"\n",
    "\n",
    "        assert input_dataset.num_chunks == len(neighbour_dataset), \\\n",
    "            \"The number of chunks in input dataset did not match the number of chunks in neighbour dataset\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_dataset.num_sequences\n",
    "\n",
    "    def __getitem__(self, seq_index: int):\n",
    "        input_chunk_indices = self.input_dataset.get_chunk_indices_of_sequence(seq_index)[:self.max_num_chunks]\n",
    "        neighbours_chunk_ids = [self.neighbour_dataset.get_neighbours_ids(\n",
    "                    chunk_index, \n",
    "                    num_neighbours=self.num_neighbours, \n",
    "                    continuation_chunks=self.continuation_chunks\n",
    "                ) for chunk_index in input_chunk_indices[:self.max_num_chunks]]\n",
    "\n",
    "        neighbour_embs = []\n",
    "        for neighbours in neighbours_chunk_ids:\n",
    "            slc_embs = []\n",
    "            for slc in neighbours:\n",
    "                if slc is not None:\n",
    "                    slc_embs.append(self.ret_embeddings[slc])\n",
    "                else:\n",
    "                    slc_embs.append(None)\n",
    "            neighbour_embs.append(slc_embs)\n",
    "\n",
    "        return self.val_embeddings[input_chunk_indices], neighbour_embs\n",
    "    \n",
    "    \n",
    "def get_retro_embedding_dataset_from_spec(\n",
    "    spec_file: Path, \n",
    "    retrieval_spec_file: Path,\n",
    "    num_neighbours=None,\n",
    "    continuation_chunks=1,\n",
    "    pad_token_idx=0,\n",
    "    max_len=None,\n",
    ") -> RetroEmbeddingDataset:\n",
    "\n",
    "    spec = json.load(spec_file.open())\n",
    "    base_dir = spec_file.parent\n",
    "\n",
    "    # input dataset\n",
    "    input_dataset = ShardedChunkedSequenceDataset([\n",
    "        ChunkedSequenceDataset(\n",
    "            chunks=base_dir / shard[\"chunks\"],\n",
    "            seq2chunk=base_dir / shard[\"seq2chunk\"],\n",
    "            chunk2seq=base_dir / shard[\"chunk2seq\"]\n",
    "        )\n",
    "        for shard in spec[\"shards\"]\n",
    "    ])\n",
    "    \n",
    "    # retrieval dataset\n",
    "    index_spec = json.load((base_dir / spec[\"neighbours\"][\"index_spec\"]).open())\n",
    "    index_base_dir = base_dir / Path(spec[\"neighbours\"][\"index_spec\"]).parent\n",
    "    retrieval_dataset = ShardedChunkedSequenceDatasetWithIDGetter([\n",
    "        ChunkedSequenceDatasetWithIDGetter(\n",
    "            chunks=index_base_dir / shard[\"chunks\"],\n",
    "            seq2chunk=index_base_dir / shard[\"seq2chunk\"],\n",
    "            chunk2seq=index_base_dir / shard[\"chunk2seq\"]\n",
    "        )\n",
    "        for shard in index_spec\n",
    "    ])\n",
    "\n",
    "    # neighbour dataset\n",
    "    neighbour_dataset = ShardedChunkNeighbourDatasetWithIDGetter([\n",
    "        ChunkNeighbourDatasetWithIDGetter(\n",
    "            neighbours=base_dir / shard[\"neighbours\"],\n",
    "            retrieval_dataset=retrieval_dataset\n",
    "        )\n",
    "        for shard in spec[\"shards\"]\n",
    "    ])\n",
    "    \n",
    "    # embeddings\n",
    "    val_emb_addrss = [base_dir / shard[\"neighbours\"].replace(\"neighbours\", \"embeddings\") \\\n",
    "                      for shard in spec[\"shards\"]]\n",
    "    val_embeddings = [np.load(emb_addrs, mmap_mode=\"r\") for emb_addrs in val_emb_addrss]\n",
    "    val_embeddings = da.concatenate(val_embeddings, axis=0)\n",
    "\n",
    "    ret_emb_addrss = [base_dir / Path(el['embeddings'].replace('../', 'retriever_sentence_transformer/')) \\\n",
    "                      for el in json.load(retrieval_spec_file.open())]\n",
    "    ret_embeddings = [np.load(emb_addrs, mmap_mode=\"r\") for emb_addrs in ret_emb_addrss]\n",
    "    ret_embeddings = da.concatenate(ret_embeddings, axis=0)\n",
    "    \n",
    "    retro_dataset = RetroEmbeddingDataset(\n",
    "        input_dataset=input_dataset,\n",
    "        neighbour_dataset=neighbour_dataset,\n",
    "        val_embeddings=val_embeddings,\n",
    "        ret_embeddings=ret_embeddings,\n",
    "        num_neighbours=num_neighbours,\n",
    "        continuation_chunks=continuation_chunks,\n",
    "        pad_token_idx=pad_token_idx,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "\n",
    "    return retro_dataset\n",
    "\n",
    "cos_ds = get_retro_embedding_dataset_from_spec(\n",
    "    spec_file=Path('retro/data/datasets/MassiveOpenText/val_sentence_transformer_neighbours.spec.json'),\n",
    "    retrieval_spec_file=Path('retro/data/datasets/MassiveOpenText/retriever_sentence_transformer/val.index.spec.json'),\n",
    "    num_neighbours=2,\n",
    "    continuation_chunks=1,\n",
    "    pad_token_idx=retro_on_config.pad_token_idx,\n",
    "    max_len=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attending_chunk_neighbour_l2_distance(inputs_emb, neighbours_emb):\n",
    "    if len(inputs_emb) <= 1 or len(neighbours_emb) == 0:\n",
    "        return []\n",
    "\n",
    "    l2_distances = []\n",
    "    for i in range(1, len(inputs_emb)):\n",
    "        distance_from_neighbors = []\n",
    "        for neighbors in neighbours_emb[i-1]:\n",
    "            if neighbors is None:\n",
    "                continue\n",
    "            for neighbor in neighbors:\n",
    "                distance_from_neighbors.append(((inputs_emb[i]-neighbor)**2).sum().compute())\n",
    "        try:\n",
    "            l2_distances.append(max(distance_from_neighbors))\n",
    "        except:\n",
    "            l2_distances.append(0.)\n",
    "        \n",
    "    return l2_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_distances = []\n",
    "for seq in tqdm(cos_ds_):\n",
    "    l2_distances.extend(get_attending_chunk_neighbour_l2_distance(*seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_retro import ChunkedSequenceDataset, RetroTrainingExample, ShardedChunkedSequenceDataset, \\\n",
    "    ChunkNeighbourDataset, ShardedChunkNeighbourDataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "  \n",
    "bm25_neighbours = np.load('retro/data/datasets/MassiveOpenText/retriever_bm25/neighbours.npy')\n",
    "\n",
    "class ChunkNeighbourDatasetBM25(ChunkNeighbourDataset):\n",
    "    def get_neighbours(self, global_idx: int, num_neighbours: int=None, continuation_chunks: int=1):\n",
    "        return [\n",
    "            self.retrieval_dataset.get_chunk_tokens(\n",
    "                neighbour_chunk_idx, \n",
    "                include_continuation_chunks=continuation_chunks\n",
    "            ) if neighbour_chunk_idx != -1 else None\n",
    "            for neighbour_chunk_idx in bm25_neighbours[conv_ids[global_idx]][:num_neighbours]\n",
    "        ]\n",
    "\n",
    "\n",
    "class ShardedChunkNeighbourDatasetBM25(ShardedChunkNeighbourDataset):\n",
    "    def get_neighbours(self, chunk_index: int, num_neighbours: int=None, continuation_chunks: int=1):\n",
    "        for shard in self.shards:\n",
    "            return shard.get_neighbours(chunk_index, num_neighbours, continuation_chunks)\n",
    "        raise IndexError(f\"Neighbours for index {chunk_index} not found\")\n",
    "\n",
    "        \n",
    "class RetroBM25Dataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dataset: ShardedChunkedSequenceDataset, \n",
    "        neighbour_dataset: ShardedChunkNeighbourDatasetBM25, \n",
    "        num_neighbours=None, \n",
    "        continuation_chunks=1, \n",
    "        pad_token_idx=0,\n",
    "        max_len=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dataset = input_dataset\n",
    "        self.neighbour_dataset = neighbour_dataset\n",
    "        self.num_neighbours = num_neighbours\n",
    "        self.continuation_chunks = continuation_chunks\n",
    "        self.neighbour_size = neighbour_dataset.chunk_size * (1 + continuation_chunks)\n",
    "        self.pad_token_idx = pad_token_idx\n",
    "        self.max_num_chunks = max_len // input_dataset.chunk_size if max_len is not None else None\n",
    "\n",
    "        if max_len is not None:\n",
    "            assert max_len % input_dataset.chunk_size == 0, \\\n",
    "                \"max_len must be a multiple of chunk_size\"\n",
    "\n",
    "        assert input_dataset.num_chunks == len(neighbour_dataset), \\\n",
    "            \"The number of chunks in input dataset did not match the number of chunks in neighbour dataset\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_dataset.num_sequences\n",
    "\n",
    "    def __getitem__(self, seq_index: int) -> RetroTrainingExample:\n",
    "        input_chunk_indices = self.input_dataset.get_chunk_indices_of_sequence(seq_index)\n",
    "\n",
    "        for idx in input_chunk_indices[:self.max_num_chunks]:\n",
    "            if idx not in conv_ids:\n",
    "                conv_ids[idx] = len(conv_ids)\n",
    "\n",
    "        # input_ids\n",
    "        input_ids = np.concatenate([\n",
    "            self.input_dataset.get_chunk_tokens(chunk_index)\n",
    "            for chunk_index in input_chunk_indices[:self.max_num_chunks]\n",
    "        ])\n",
    "\n",
    "        # neighbour_ids\n",
    "        neighbour_ids = np.stack([\n",
    "            [\n",
    "                np.pad(neighbour_tokens, (0, self.neighbour_size - len(neighbour_tokens)), constant_values=self.pad_token_idx) \\\n",
    "                    if neighbour_tokens is not None else \\\n",
    "                np.ones(self.neighbour_size) * self.pad_token_idx\n",
    "\n",
    "                for neighbour_tokens in self.neighbour_dataset.get_neighbours(\n",
    "                    chunk_index, \n",
    "                    num_neighbours=self.num_neighbours, \n",
    "                    continuation_chunks=self.continuation_chunks\n",
    "                )\n",
    "            ]\n",
    "            for chunk_index in input_chunk_indices[:self.max_num_chunks]\n",
    "        ])\n",
    "\n",
    "        # labels - set to -100 at padded tokens\n",
    "        labels = np.pad(input_ids[1:], (0, 1), constant_values=self.pad_token_idx).astype(np.int64)\n",
    "        labels[labels == self.pad_token_idx] = -100\n",
    "\n",
    "        return RetroTrainingExample(\n",
    "            torch.from_numpy(input_ids.astype(np.int32)), \n",
    "            torch.from_numpy(neighbour_ids.astype(np.int32)), \n",
    "            torch.from_numpy(labels)\n",
    "        )\n",
    "        \n",
    "def get_retro_bm25_dataset_from_spec(\n",
    "    spec_file: Path, \n",
    "    num_neighbours=None,\n",
    "    continuation_chunks=1,\n",
    "    pad_token_idx=0,\n",
    "    max_len=None,\n",
    "):\n",
    "\n",
    "    spec = json.load(spec_file.open())\n",
    "    base_dir = spec_file.parent\n",
    "\n",
    "    # input dataset\n",
    "    input_dataset = ShardedChunkedSequenceDataset([\n",
    "        ChunkedSequenceDataset(\n",
    "            chunks=base_dir / shard[\"chunks\"],\n",
    "            seq2chunk=base_dir / shard[\"seq2chunk\"],\n",
    "            chunk2seq=base_dir / shard[\"chunk2seq\"]\n",
    "        )\n",
    "        for shard in spec[\"shards\"]\n",
    "    ])\n",
    "\n",
    "    # retrieval dataset\n",
    "    index_spec = json.load((base_dir / spec[\"neighbours\"][\"index_spec\"]).open())\n",
    "    index_base_dir = base_dir / Path(spec[\"neighbours\"][\"index_spec\"]).parent\n",
    "    retrieval_dataset = ShardedChunkedSequenceDataset([\n",
    "        ChunkedSequenceDataset(\n",
    "            chunks=index_base_dir / shard[\"chunks\"],\n",
    "            seq2chunk=index_base_dir / shard[\"seq2chunk\"],\n",
    "            chunk2seq=index_base_dir / shard[\"chunk2seq\"]\n",
    "        )\n",
    "        for shard in index_spec\n",
    "    ])\n",
    "\n",
    "    # neighbour dataset\n",
    "    neighbour_dataset = ShardedChunkNeighbourDatasetBM25([\n",
    "        ChunkNeighbourDatasetBM25(\n",
    "            neighbours=base_dir / shard[\"neighbours\"],\n",
    "            retrieval_dataset=retrieval_dataset\n",
    "        )\n",
    "        for shard in spec[\"shards\"]\n",
    "    ])\n",
    "\n",
    "    retro_dataset = RetroBM25Dataset(\n",
    "        input_dataset=input_dataset,\n",
    "        neighbour_dataset=neighbour_dataset,\n",
    "        num_neighbours=num_neighbours,\n",
    "        continuation_chunks=continuation_chunks,\n",
    "        pad_token_idx=pad_token_idx,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return retro_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_neighbours.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_val_ds = get_retro_bm25_dataset_from_spec(\n",
    "    spec_file=Path('retro/data/datasets/MassiveOpenText/val_sentence_transformer_neighbours.spec.json'),\n",
    "    num_neighbours=2,\n",
    "    continuation_chunks=1,\n",
    "    pad_token_idx=retro_on_config.pad_token_idx,\n",
    "    max_len=1024\n",
    "    )\n",
    "\n",
    "bm25_val_dl = DataLoader(bm25_val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptureLossesHook(pl.Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "    \n",
    "    def on_test_batch_end(self, trainer, module, outputs, *args):\n",
    "        loss = outputs\n",
    "        self.losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RETRO\\[OFF\\]:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "retro_off_config = RetroConfig(**json.load(Path('retro/data/model/retro.json').open()))\n",
    "retro_off_config.dec_cca_layers = []\n",
    "retro_off_model = RetroModelLMHeadLightning(retro_off_config)\n",
    "CHECKPOINT_PATH = 'retro/data/model/retro_model.ckpt'\n",
    "\n",
    "retro_off_loss_hook = CaptureLossesHook()\n",
    "retro_off_model = RetroModelLMHeadLightning.load_from_checkpoint(str(CHECKPOINT_PATH), config=retro_off_config, strict=False).eval()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=-1,\n",
    "    logger=False,\n",
    "    callbacks=[retro_off_loss_hook]\n",
    ")\n",
    "\n",
    "trainer.test(retro_off_model, dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RETRO\\[ON\\]-ST:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retro_on_model = RetroModelLMHeadLightning(retro_on_config)\n",
    "retro_on_loss_hook = CaptureLossesHook()\n",
    "trainer = pl.Trainer(\n",
    "    gpus=-1, \n",
    "    logger=False,\n",
    "    callbacks=[retro_on_loss_hook]\n",
    ")\n",
    "\n",
    "trainer.test(retro_on_model, dataloaders=val_dl, ckpt_path=CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RETRO\\[ON\\]-BM25:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in bm25_val_ds:\n",
    "    pass\n",
    "\n",
    "retro_on_bm25_model = RetroModelLMHeadLightning(retro_on_config)\n",
    "retro_on_bm25_loss_hook = CaptureLossesHook()\n",
    "trainer = pl.Trainer(\n",
    "    gpus=-1,\n",
    "    logger=False,\n",
    "    callbacks=[retro_on_bm25_loss_hook]\n",
    ")\n",
    "\n",
    "trainer.test(retro_on_bm25_model, dataloaders=bm25_val_dl, ckpt_path=CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap, total = 0, 0\n",
    "for i in range(len(val_ds)):\n",
    "    bert_ns = val_ds[i].neighbour_ids\n",
    "    bm25_ns = bm25_val_ds[i].neighbour_ids\n",
    "    overlap += sum((bert_ns[:,0] == bm25_ns[:,0]).all(axis=-1))\n",
    "    total += bert_ns.shape[0]\n",
    "    \n",
    "print(overlap/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq: list, chunk_size: int=64):\n",
    "    for i in range(0, len(seq), chunk_size):\n",
    "        yield seq[i:i + chunk_size]\n",
    "\n",
    "retro_on_losses = [loss for losses in retro_on_loss_hook.losses for loss in losses]\n",
    "retro_off_losses = [loss for losses in retro_off_loss_hook.losses for loss in losses]\n",
    "\n",
    "loss_per_chunk = []\n",
    "\n",
    "for i in range(len(val_ds)):\n",
    "    ids_ = val_ds[i].input_ids.nonzero().reshape(-1)\n",
    "    retro_on_chunks = chunker(retro_on_losses[i][ids_].cpu())\n",
    "    retro_off_chunks = chunker(retro_off_losses[i][ids_].cpu())\n",
    "    for j, (on_chunk, off_chunk) in enumerate(zip(retro_on_chunks, retro_off_chunks)):\n",
    "        if j == 0:\n",
    "            continue\n",
    "        loss_per_chunk.append((off_chunk - on_chunk).mean())\n",
    "print(len(loss_per_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retro_on_losses = torch.concat(list(batch_loss.mean(-1) for batch_loss in retro_on_loss_hook.losses)).cpu().detach().numpy()\n",
    "retro_off_losses = torch.concat(list(batch_loss.mean(-1) for batch_loss in retro_off_loss_hook.losses)).cpu().detach().numpy()\n",
    "avg_loss_diff = retro_off_losses - retro_on_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(spearmanr(u_overlaps, np.exp(loss_per_chunk)))\n",
    "print(pearsonr(u_overlaps, np.exp(loss_per_chunk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_distances = np.nan_to_num(l2_distances, neginf=0) \n",
    "print(spearmanr(-l2_distances, np.exp(loss_per_chunk)))\n",
    "print(pearsonr(-l2_distances, np.exp(loss_per_chunk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spearmanr(-l2_distances, u_overlaps))\n",
    "print(pearsonr(-l2_distances, u_overlaps))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c86c3c4d286c521bff614c57c245c0cf9059fa3b7a034d6a436d32672e247c13"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
